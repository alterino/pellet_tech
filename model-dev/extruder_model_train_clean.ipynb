{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_start_of_daily_block(date_series):\n",
    "    \"\"\"\n",
    "    Returns a boolean Series indicating if each date in the input Series is the \n",
    "    start of a new sequence of consecutive dates.\n",
    "\n",
    "    :param date_series: Pandas Series with datetime objects.\n",
    "    :return: Pandas Series of boolean values.\n",
    "    \"\"\"\n",
    "    # Ensure the Series is sorted\n",
    "    sorted_series = date_series.sort_values()\n",
    "\n",
    "    # Calculate the difference in days between each date and the previous date\n",
    "    day_diff = sorted_series.diff().dt.days\n",
    "\n",
    "    # Mark True where the difference is greater than 1 day, indicating a new sequence\n",
    "    is_start = day_diff > 1\n",
    "\n",
    "    # Handle the first element\n",
    "    if not is_start.empty:\n",
    "        is_start.iloc[0] = True\n",
    "\n",
    "    return is_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df[(df['Extruder Pressure'] >= 0) & (df['Extruder Pressure'] < 50000)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine_pauses(df, time_threshold=30, index=True):\n",
    "    if index:\n",
    "        time_diff = -1*(df.index.diff(-1).dt.total_seconds()/60)\n",
    "        machine_pauses = df[time_diff > time_threshold].index\n",
    "    else:\n",
    "        time_diff = -1*(df['Date and Time'].diff(-1).dt.total_seconds()/60)\n",
    "        machine_pauses = df[time_diff > time_threshold]['Date and Time']\n",
    "    \n",
    "    return machine_pauses\n",
    "\n",
    "def calculate_time_difference(df, time_threshold=30):\n",
    "    time_diff = -1*(df['Date and Time'].diff(-1).dt.total_seconds()/60)\n",
    "    #time_diff = (df['Date and Time'].shift(-1) - df['Date and Time']).dt.total_seconds()/60\n",
    "\n",
    "    return time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extruder_data(extruder_data):\n",
    "    \n",
    "    extruder_data = extruder_data.drop(extruder_data.index[0])\n",
    "    extruder_data = extruder_data.replace(\"<null>\", None)\n",
    "    \n",
    "    extruder_data = extruder_data.dropna()\n",
    "\n",
    "    # Convert the column to string type\n",
    "    for column in extruder_data.columns:\n",
    "        if column not in [\"Date and Time\", \"AnomalyScore\"]:\n",
    "            extruder_data[column] = extruder_data[column].astype(str)\n",
    "            #extruder_data[column] = extruder_data[column].str.replace(\"<null>\", None)\n",
    "            # Replace commas with an empty string\n",
    "            extruder_data[column] = extruder_data[column].str.replace(\",\", \"\")\n",
    "\n",
    "            # Convert the column back to numeric type, handling any conversion errors\n",
    "            extruder_data[column] = pd.to_numeric(extruder_data[column], errors=\"coerce\")\n",
    "            \n",
    "    extruder_data = extruder_data[(extruder_data['Extruder Pressure'] >= 0) & \\\n",
    "                              (extruder_data['Extruder Pressure'] < 50000)]\n",
    "\n",
    "    extruder_data['Date and Time'] = pd.to_datetime(extruder_data['Date and Time'])\n",
    "    \n",
    "    return extruder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47197/3404226215.py:6: DtypeWarning: Columns (2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ex4_data = clean_extruder_data( pd.read_csv(\"ex4-data.csv\") )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "columns_oi = ['Extruder Pressure']\n",
    "\n",
    "ex4_data = clean_extruder_data( pd.read_csv(\"ex4-data.csv\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex4_data = ex4_data.groupby('Date and Time').mean()\n",
    "\n",
    "# resampling every 5 minutes instead of 1 minute\n",
    "\n",
    "ex4_resampled = ex4_data.resample('5T').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ex4_data['Date and Time'] = ex4_data.index\n",
    "ex4_pauses = get_machine_pauses(ex4_data, time_threshold=15, index=False)\n",
    "\n",
    "pauses = ex4_pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_since_last_pause(time, pauses):\n",
    "    return (time - pauses[pauses <= time].max()).total_seconds()/3600\n",
    "\n",
    "def hours_to_next_pause(time, pauses):\n",
    "    return (pauses[pauses >= time].min() - time).total_seconds()/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 844 Âµs, total: 47.6 s\n",
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#from tqdm import tqdm\n",
    "\n",
    "ex4_data['hours_since_last_pause'] = ex4_data.index.to_series().apply(\n",
    "    lambda x : hours_since_last_pause(x, pauses)\n",
    ")\n",
    "ex4_data['hours_to_next_pause'] = ex4_data.index.to_series().apply(\n",
    "    lambda x : hours_to_next_pause(x, pauses)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for time_window in ['15T', '30T', '1H', '2H', '3H', '6H', '12H']:\n",
    "    if 'T' in time_window:\n",
    "        label_append = time_window.replace('T', 'min')\n",
    "    elif 'H' in time_window:\n",
    "        label_append = time_window.replace('H', 'hour')\n",
    "\n",
    "    col_label = 'pressure-' + label_append + '_avg'\n",
    "    ex4_data[col_label] = ex4_data['Extruder Pressure'].rolling(time_window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_lag = ['Extruder Die Temp', 'Extruder Thrust',\n",
    "                    'Screw Speed Output']\n",
    "\n",
    "colname_map = {\n",
    "    'Extruder Die Temp' : 'die_temp-',\n",
    "    'Extruder Thrust' : 'thrust-',\n",
    "    'Screw Speed Output' : 'screw_speed_output-',\n",
    "}\n",
    "\n",
    "for time_window in ['15T', '1H', '3H']:\n",
    "    if 'T' in time_window:\n",
    "        label_append = time_window.replace('T', 'min')\n",
    "    elif 'H' in time_window:\n",
    "        label_append = time_window.replace('H', 'hour')\n",
    "    for lagvar in variables_to_lag:\n",
    "        col_label = colname_map[lagvar] + label_append + '_avg'\n",
    "        ex4_data[col_label] = \\\n",
    "            ex4_data[lagvar].rolling(time_window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_datetime = pauses.min() - pd.Timedelta(days=7)\n",
    "max_datetime = pauses.max()\n",
    "\n",
    "time_mask = (ex4_data.index >= min_datetime) \\\n",
    "            & (ex4_data.index <= max_datetime)\n",
    "ex4_data = ex4_data[time_mask]\n",
    "\n",
    "redundancy_mask = (ex4_data['hours_since_last_pause'] >= 3) \\\n",
    "                  | (ex4_data['hours_to_next_pause'] >= 3)\n",
    "ex4_data = ex4_data[redundancy_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up classifier variables -\n",
    "# whether there is a pause within \n",
    "# {1 minute, 5 minutes, 10 minutes, 15 minutes, 30 minutes,\n",
    "#  1 hour, 2 hours, 3 hours, 6 hours, 12 hours, 24 hours}\n",
    "\n",
    "stop_target_to_hours_map = {\n",
    "#    '1min' : 1/60,\n",
    "    '10min': 10/60,\n",
    "    '15min': 15/60,\n",
    "    '30min': 30/60,\n",
    "    '1hour': 1,\n",
    "    '2hour': 2,\n",
    "    '3hour': 3,\n",
    "    '6hour':6,\n",
    "    '12hour':12,\n",
    "    '24hour':24,\n",
    "}\n",
    "\n",
    "\n",
    "for window in stop_target_to_hours_map:\n",
    "    ex4_data[f'{window}_stop'] = \\\n",
    "        ex4_data['hours_to_next_pause'] < stop_target_to_hours_map[window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def balance_dataset(X, y, over_sample_ratio=0.1, under_sample_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Balances the dataset by over-sampling the minority class and under-sampling the majority class.\n",
    "\n",
    "    :param X: Feature set.\n",
    "    :param y: Labels.\n",
    "    :param over_sample_ratio: Ratio of the number of samples in the minority class after resampling\n",
    "                              to the number of samples in the majority class before resampling.\n",
    "    :param under_sample_ratio: Ratio of the number of samples in the majority class after resampling\n",
    "                               to the number of samples in the minority class after over-sampling.\n",
    "\n",
    "    :return: Balanced features and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the resampling strategy\n",
    "    over = SMOTE(sampling_strategy=over_sample_ratio)\n",
    "    under = RandomUnderSampler(sampling_strategy=under_sample_ratio)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # Transform the dataset\n",
    "    X_balanced, y_balanced = pipeline.fit_resample(X, y)\n",
    "\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = [s+ '_stop' for s in stop_target_to_hours_map.keys()]\n",
    "input_features = [\n",
    "    'Screw Speed Output',\n",
    "    'Screw Speed',\n",
    "    'Extruder Die Temp',\n",
    "    'Extruder Thrust',\n",
    "    'Feed Screw Current (Amps)',\n",
    "    'Discharge Conveyor Current (Amps)',\n",
    "    'Discharge Conveyor Speed (%)',\n",
    "    'pressure-15min_avg',\n",
    "    'pressure-30min_avg',\n",
    "    'pressure-1hour_avg',\n",
    "    'pressure-2hour_avg',\n",
    "    'pressure-3hour_avg',\n",
    "    'die_temp-15min_avg',\n",
    "    'thrust-15min_avg',\n",
    "    'die_temp-1hour_avg',\n",
    "    'thrust-1hour_avg',\n",
    "    'die_temp-3hour_avg',\n",
    "    'thrust-3hour_avg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = ex4_data[input_features]\n",
    "model_output = ex4_data[target_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input variables:\n",
      "Screw Speed Output\n",
      "Screw Speed\n",
      "Extruder Die Temp\n",
      "Extruder Thrust\n",
      "Feed Screw Current (Amps)\n",
      "Discharge Conveyor Current (Amps)\n",
      "Discharge Conveyor Speed (%)\n",
      "pressure-15min_avg\n",
      "pressure-30min_avg\n",
      "pressure-1hour_avg\n",
      "pressure-2hour_avg\n",
      "pressure-3hour_avg\n",
      "die_temp-15min_avg\n",
      "thrust-15min_avg\n",
      "die_temp-1hour_avg\n",
      "thrust-1hour_avg\n",
      "die_temp-3hour_avg\n",
      "thrust-3hour_avg\n",
      "original true samples: 69, original false samples: 36505\n",
      "balanced true samples: 3650, balanced false samples: 7300\n",
      "\n",
      "Current Time: 22:01:13\n",
      "results for target: 10min_stop, model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.9353674540682415, 'Cross-Val Mean': 0.9968284137839436, 'Confusion_Matrix': array([[8539,  580],\n",
      "       [  11,   14]])}\n",
      "--------------------------------------\n",
      "Run time: 0.03 minutes\n",
      "Current Time: 22:01:15\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:01:15\n",
      "results for target: 10min_stop, model: Random Forest\n",
      "{'Accuracy': 0.9977034120734908, 'Cross-Val Mean': 0.8283682237002449, 'Confusion_Matrix': array([[9101,   18],\n",
      "       [   3,   22]])}\n",
      "--------------------------------------\n",
      "Run time: 0.76 minutes\n",
      "Current Time: 22:01:58\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:01:58\n",
      "results for target: 10min_stop, model: Support Vector Machine\n",
      "{'Accuracy': 0.9946412948381452, 'Cross-Val Mean': 0.9979439184861818, 'Confusion_Matrix': array([[9086,   33],\n",
      "       [  16,    9]])}\n",
      "--------------------------------------\n",
      "Run time: 0.96 minutes\n",
      "Current Time: 22:02:10\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:02:10\n",
      "results for target: 10min_stop, model: K-Nearest Neighbors\n",
      "{'Accuracy': 0.9800962379702537, 'Cross-Val Mean': 0.9093174728664769, 'Confusion_Matrix': array([[8941,  178],\n",
      "       [   4,   21]])}\n",
      "--------------------------------------\n",
      "Run time: 1.03 minutes\n",
      "Current Time: 22:02:15\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:02:15\n",
      "results for target: 10min_stop, model: Gradient Boosting\n",
      "{'Accuracy': 0.9944225721784777, 'Cross-Val Mean': 0.8220251230353834, 'Confusion_Matrix': array([[9073,   46],\n",
      "       [   5,   20]])}\n",
      "--------------------------------------\n",
      "Run time: 2.08 minutes\n",
      "Current Time: 22:03:17\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:03:17\n",
      "results for target: 10min_stop, model: MLP\n",
      "{'Accuracy': 0.22397200349956256, 'Cross-Val Mean': 0.9972221121791035, 'Confusion_Matrix': array([[2026, 7093],\n",
      "       [   3,   22]])}\n",
      "--------------------------------------\n",
      "Run time: 2.45 minutes\n",
      "Current Time: 22:03:40\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "original true samples: 102, original false samples: 36472\n",
      "balanced true samples: 3647, balanced false samples: 7294\n",
      "\n",
      "Current Time: 22:03:40\n",
      "results for target: 15min_stop, model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.9375546806649169, 'Cross-Val Mean': 0.995341078168029, 'Confusion_Matrix': array([[8558,  553],\n",
      "       [  18,   15]])}\n",
      "--------------------------------------\n",
      "Run time: 0.04 minutes\n",
      "Current Time: 22:03:42\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:03:42\n",
      "results for target: 15min_stop, model: Random Forest\n",
      "{'Accuracy': 0.9973753280839895, 'Cross-Val Mean': 0.8226813077600781, 'Confusion_Matrix': array([[9091,   20],\n",
      "       [   4,   29]])}\n",
      "--------------------------------------\n",
      "Run time: 0.60 minutes\n",
      "Current Time: 22:04:16\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:04:16\n",
      "results for target: 15min_stop, model: Support Vector Machine\n",
      "{'Accuracy': 0.9944225721784777, 'Cross-Val Mean': 0.9970471149134358, 'Confusion_Matrix': array([[9085,   26],\n",
      "       [  25,    8]])}\n",
      "--------------------------------------\n",
      "Run time: 0.82 minutes\n",
      "Current Time: 22:04:29\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:04:29\n",
      "results for target: 15min_stop, model: K-Nearest Neighbors\n",
      "{'Accuracy': 0.9832677165354331, 'Cross-Val Mean': 0.8659884757104696, 'Confusion_Matrix': array([[8960,  151],\n",
      "       [   2,   31]])}\n",
      "--------------------------------------\n",
      "Run time: 0.95 minutes\n",
      "Current Time: 22:04:37\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:04:37\n",
      "results for target: 15min_stop, model: Gradient Boosting\n",
      "{'Accuracy': 0.9944225721784777, 'Cross-Val Mean': 0.8220907589712174, 'Confusion_Matrix': array([[9061,   50],\n",
      "       [   1,   32]])}\n",
      "--------------------------------------\n",
      "Run time: 2.08 minutes\n",
      "Current Time: 22:05:45\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:05:45\n",
      "results for target: 15min_stop, model: MLP\n",
      "{'Accuracy': 0.9248687664041995, 'Cross-Val Mean': 0.9966315466445508, 'Confusion_Matrix': array([[8442,  669],\n",
      "       [  18,   15]])}\n",
      "--------------------------------------\n",
      "Run time: 2.42 minutes\n",
      "Current Time: 22:06:05\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "original true samples: 207, original false samples: 36367\n",
      "balanced true samples: 3636, balanced false samples: 7272\n",
      "\n",
      "Current Time: 22:06:05\n",
      "results for target: 30min_stop, model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.8834208223972003, 'Cross-Val Mean': 0.9920599081436595, 'Confusion_Matrix': array([[8051, 1028],\n",
      "       [  38,   27]])}\n",
      "--------------------------------------\n",
      "Run time: 0.06 minutes\n",
      "Current Time: 22:06:08\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:06:08\n",
      "results for target: 30min_stop, model: Random Forest\n",
      "{'Accuracy': 0.997594050743657, 'Cross-Val Mean': 0.8136696327593198, 'Confusion_Matrix': array([[9060,   19],\n",
      "       [   3,   62]])}\n",
      "--------------------------------------\n",
      "Run time: 0.69 minutes\n",
      "Current Time: 22:06:47\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:06:47\n",
      "results for target: 30min_stop, model: Support Vector Machine\n",
      "{'Accuracy': 0.992344706911636, 'Cross-Val Mean': 0.9940504852949381, 'Confusion_Matrix': array([[9066,   13],\n",
      "       [  57,    8]])}\n",
      "--------------------------------------\n",
      "Run time: 0.87 minutes\n",
      "Current Time: 22:06:57\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:06:57\n",
      "results for target: 30min_stop, model: K-Nearest Neighbors\n",
      "{'Accuracy': 0.9772528433945756, 'Cross-Val Mean': 0.8236870013910407, 'Confusion_Matrix': array([[8879,  200],\n",
      "       [   8,   57]])}\n",
      "--------------------------------------\n",
      "Run time: 0.94 minutes\n",
      "Current Time: 22:07:01\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:07:01\n",
      "results for target: 30min_stop, model: Gradient Boosting\n",
      "{'Accuracy': 0.9903762029746281, 'Cross-Val Mean': 0.8064067988849091, 'Confusion_Matrix': array([[8995,   84],\n",
      "       [   4,   61]])}\n",
      "--------------------------------------\n",
      "Run time: 2.01 minutes\n",
      "Current Time: 22:08:06\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:08:06\n",
      "results for target: 30min_stop, model: MLP\n",
      "{'Accuracy': 0.06528871391076116, 'Cross-Val Mean': 0.9912506582253069, 'Confusion_Matrix': array([[ 533, 8546],\n",
      "       [   1,   64]])}\n",
      "--------------------------------------\n",
      "Run time: 2.30 minutes\n",
      "Current Time: 22:08:23\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "original true samples: 372, original false samples: 36202\n",
      "balanced true samples: 3620, balanced false samples: 7240\n",
      "\n",
      "Current Time: 22:08:23\n",
      "results for target: 1hour_stop, model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/michael/.virtualenvs/default/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.8841863517060368, 'Cross-Val Mean': 0.9842509494089681, 'Confusion_Matrix': array([[8036,  998],\n",
      "       [  61,   49]])}\n",
      "--------------------------------------\n",
      "Run time: 0.04 minutes\n",
      "Current Time: 22:08:25\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:08:25\n",
      "results for target: 1hour_stop, model: Random Forest\n",
      "{'Accuracy': 0.9970472440944882, 'Cross-Val Mean': 0.8018794407780948, 'Confusion_Matrix': array([[9011,   23],\n",
      "       [   4,  106]])}\n",
      "--------------------------------------\n",
      "Run time: 0.55 minutes\n",
      "Current Time: 22:08:56\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:08:56\n",
      "results for target: 1hour_stop, model: Support Vector Machine\n",
      "{'Accuracy': 0.9875328083989501, 'Cross-Val Mean': 0.989457108493616, 'Confusion_Matrix': array([[9024,   10],\n",
      "       [ 104,    6]])}\n",
      "--------------------------------------\n",
      "Run time: 0.77 minutes\n",
      "Current Time: 22:09:09\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:09:09\n",
      "results for target: 1hour_stop, model: K-Nearest Neighbors\n",
      "{'Accuracy': 0.9630358705161854, 'Cross-Val Mean': 0.819946652531389, 'Confusion_Matrix': array([[8706,  328],\n",
      "       [  10,  100]])}\n",
      "--------------------------------------\n",
      "Run time: 0.84 minutes\n",
      "Current Time: 22:09:14\n",
      "--------------------------------------\n",
      "\n",
      "Current Time: 22:09:14\n",
      "results for target: 1hour_stop, model: Gradient Boosting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(Y_test, predictions)\n\u001b[1;32m     77\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, predictions)\n\u001b[0;32m---> 78\u001b[0m cross_val \u001b[38;5;241m=\u001b[39m cross_val_score(model, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     79\u001b[0m results[targ][name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross-Val Mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(cross_val),\n\u001b[1;32m     80\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion_Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m : conf_matrix}\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[targ][name])\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:247\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    246\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    252\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    260\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/default/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Load a dataset (Iris dataset as an example)\n",
    "#data = load_iris()\n",
    "X = model_input.values\n",
    "\n",
    "print('input variables:')\n",
    "for var in model_input.columns:\n",
    "    print(var)\n",
    "\n",
    "# Define the models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'MLP' : MLPClassifier(hidden_layer_sizes=(16, 32), max_iter=1000)\n",
    "}\n",
    "\n",
    "# Dictionary to hold the results\n",
    "results = {}\n",
    "output_models = {}\n",
    "\n",
    "saved_processed_data = {}\n",
    "\n",
    "# Iterate over the models, fit, and evaluate them\n",
    "for targ in target_features:\n",
    "    if targ not in results:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results[targ] = dict()\n",
    "        output_models[targ] = dict()\n",
    "    y = model_output[targ].values\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    n_true = np.sum((Y_train == 1))\n",
    "    n_false = np.sum((Y_train == 0))\n",
    "    print(f'original true samples: {n_true}, original false samples: {n_false}')\n",
    "    try:\n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "    except Exception as e:\n",
    "        print(f'errored on balanced dataset: {e}, using raw dataset')\n",
    "    \n",
    "    saved_processed_data[targ] = (X_train, Y_train)\n",
    "    \n",
    "    n_true = np.sum((Y_train == 1))\n",
    "    n_false = np.sum((Y_train == 0))\n",
    "    print(f'balanced true samples: {n_true}, balanced false samples: {n_false}')\n",
    "    for name, model in models.items():\n",
    "        print()\n",
    "        print(f\"Current Time: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "        print(f'results for target: {targ}, model: {name}')\n",
    "        model.fit(X_train, Y_train)\n",
    "        output_models[targ][name] = model\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "        accuracy = accuracy_score(Y_test, predictions)\n",
    "        cross_val = cross_val_score(model, X, y, cv=5)\n",
    "        results[targ][name] = {'Accuracy': accuracy, 'Cross-Val Mean': np.mean(cross_val),\n",
    "                              'Confusion_Matrix' : conf_matrix}\n",
    "        \n",
    "        \n",
    "        print(results[targ][name])\n",
    "        print('--------------------------------------')\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Run time: {duration / 60:.2f} minutes\")\n",
    "        print(f\"Current Time: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "        print('--------------------------------------')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "with open('models_no_screw_speed_no_5T_ex4.pkl') as f:\n",
    "    pickle.dump((output_models, results), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = [s+ '_stop' for s in stop_target_to_hours_map.keys()]\n",
    "input_features = [\n",
    "    'Screw Speed Output',\n",
    "    'Screw Speed',\n",
    "    'Extruder Die Temp',\n",
    "    'Extruder Thrust',\n",
    "    'Feed Screw Current (Amps)',\n",
    "    'Discharge Conveyor Current (Amps)',\n",
    "    'Discharge Conveyor Speed (%)',\n",
    "    'pressure-15min_avg',\n",
    "    'pressure-30min_avg',\n",
    "    'pressure-1hour_avg',\n",
    "    'pressure-2hour_avg',\n",
    "    'pressure-3hour_avg',\n",
    "    'die_temp-15min_avg',\n",
    "    'thrust-15min_avg',\n",
    "    'screw_speed_output-15min_avg',\n",
    "    'die_temp-1hour_avg',\n",
    "    'thrust-1hour_avg',\n",
    "    'screw_speed_output-1hour_avg',\n",
    "    'die_temp-3hour_avg',\n",
    "    'thrust-3hour_avg',\n",
    "    'screw_speed_output-3hour_avg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = ex4_data[input_features]\n",
    "model_output = ex4_data[target_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset (Iris dataset as an example)\n",
    "#data = load_iris()\n",
    "X = model_input.values\n",
    "\n",
    "print('input variables:')\n",
    "for var in model_input.columns:\n",
    "    print(var)\n",
    "#y = model_output.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'MLP' : MLPClassifier(hidden_layer_sizes=(16, 32), max_iter=1000)\n",
    "}\n",
    "\n",
    "# Dictionary to hold the results\n",
    "results = {}\n",
    "output_models = {}\n",
    "\n",
    "saved_processed_data = {}\n",
    "\n",
    "# Iterate over the models, fit, and evaluate them\n",
    "for targ in target_features:\n",
    "    if targ not in results:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results[targ] = dict()\n",
    "        output_models[targ] = dict()\n",
    "    y = model_output[targ].values\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    n_true = np.sum((Y_train == 1))\n",
    "    n_false = np.sum((Y_train == 0))\n",
    "    print(f'original true samples: {n_true}, original false samples: {n_false}')\n",
    "    try:\n",
    "        X_train, Y_train = balance_dataset(X_train, Y_train)\n",
    "    except Exception as e:\n",
    "        print(f'errored on balanced dataset: {e}, using raw dataset')\n",
    "    \n",
    "    saved_processed_data[targ] = (X_train, Y_train)\n",
    "    \n",
    "    n_true = np.sum((Y_train == 1))\n",
    "    n_false = np.sum((Y_train == 0))\n",
    "    print(f'balanced true samples: {n_true}, balanced false samples: {n_false}')\n",
    "    for name, model in models.items():\n",
    "        print()\n",
    "        print(f\"Current Time: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "        print(f'results for target: {targ}, model: {name}')\n",
    "        model.fit(X_train, Y_train)\n",
    "        output_models[targ][name] = model\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Compute the confusion matrix\n",
    "        conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "        # Visualize the confusion matrix\n",
    "        accuracy = accuracy_score(Y_test, predictions)\n",
    "        cross_val = cross_val_score(model, X, y, cv=5)\n",
    "        results[targ][name] = {'Accuracy': accuracy, 'Cross-Val Mean': np.mean(cross_val),\n",
    "                              'Confusion_Matrix' : conf_matrix}\n",
    "        \n",
    "        \n",
    "        print(results[targ][name])\n",
    "        print('--------------------------------------')\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Run time: {duration / 60:.2f} minutes\")\n",
    "        print(f\"Current Time: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "        print('--------------------------------------')\n",
    "        \n",
    "    results_df = pd.DataFrame(results[targ]).T\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "with open('models_with_screw_speed_no_5T_ex4.pkl') as f:\n",
    "    pickle.dump((output_models, results), f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
